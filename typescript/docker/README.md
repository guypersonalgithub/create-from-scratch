Docker virtualizes the operation system's application layer, is much smaller and faster to boot up compared to a virtual machine, as it doesn't contain the operating system's kernel, which is the layer that comminucates with the computer's hardware.

In order to easily run a Docker locally, we need to install Docker Desktop - https://www.docker.com/products/docker-desktop
By installing Docker Desktop we get access to an easy to use GUI, the Docker CLI and more potential tools that would make using Docker easier.

https://docs.docker.com/engine/reference/builder has all Dockerfile commands listed with explanations for each of them.

## Common terms:

- Docker image - An application artifact that contains the application's source code, the complete environment configuration and all tools and services required to run the application, alongside the operation system's application layer (which for Docker is Linux). It can also contain environment variables, the application's directory structure, files, etc. The docker image is an immutable template that defines how a Docker container will run.
- Docker container - A running instance of an image. A single Docker image can be ran by multiple containers, in order to potentially increase the application's performance or other potential use cases.
- Docker registry - A storage for predefined Docker images. There are both official images, and non-official ones.The official images are maintained by their authors in collaboration with the Docker community. (Like a Postgres based image, Mongo, Redis, etc). Docker itself hosts the biggest Docker registry - Docker Hub - https://hub.docker.com. (Registration isn't required if we only want to download images). Docker Hub is both a public and private Docker registry, so everyone can access public images and download them, unless we sign up to Docker Hub and upload the images to a private registry. If privacy matters and other options are more desired, all big cloud providers offer private registries, where authentication is required before access to the registry is granted.
- Port binding - A way to access a currently running Docker container by binding it to a port that would expose it to the host (the machine the container runs on). Generally, Docker containers run their applications in an isolated Docker network, which, in return, allows us to run the same application on the same port multiple times. In other words, We bind the container's port to the host's port to make the running service available to the outside world.
- Docker repository - A collection of related images with the same name but different versions.
- Dockerfile - A file with a set of commands that assemble an image from our application. Docker files start from a parent image or a base image. Dockerfiles must begin with a FROM instruction, which builds the image from a specified base image. Every image consists of multiple image layers, which makes Docker efficient, as image layers can be cached. As most Docker images are Linux based, we can write Linux commands inside the Dockerfile we want to excute inside the container, by using the RUN command. COPY is another command that copies application files from host into the container, thus allowing us to execute file-based commands such as npm install. Each instruction within the Dockerfile creates one layer. The layers are stacked, and each one is a delta of the changes from the previous layer. Since Docker caches the layers behind the scenes, in order to avoid invalidating the cache, we would rather minimize the number of layers by chaining multiple commands to the same layer. For example, RUN apt update && apt install nodejs -y would run both commmands under the same layer, instead of having one layer for each.
- Docker base image - mostly a lightweight Linux operating system image that has the tools that are required to be available. Base images are like any other image.
- Docker volume - Generally, any data created or modified in containers during their run time will be gone if the containers are stopped or removed. If we need data that is generated by the application to be persisted, and we don't want it to exist within the container's filesystem, a Docker volume should be used to store the data outside of the container's filesystem. That way, when we run a container, we can tell Docker to use the volume and mount it at a specific path inside the container.
- Bind mount - The data is still stored within the virtual machine, on the Linux file system, and we connect our host filesystem into the container at run time. That way, if we modify any container files that are on the path created to connect the host and the container, the changes will show up on the host filesystem. Docker suggests Docker volumes by default as opposed to Bind mounting, as bind mount is crossing the boundaries between our virtual machine and the host system, performance can be much lower if we have alot of actions that create changes or require to constantly read and write data. Since in local development performance is less relevant, as the container isn't expected to serve millions or billions of requests, through bind mount, we can mount our source code into the container so that when we make changes in our code, those changes will also show up in the container system's filesystem right away (which can help us develop with hot reloading within a Docker container).
- dockerignore - Similarly to gitignore, is a file that tells Docker which files to ignore when copying files into the container images.
- Multi stage build - Allows us to have within a single Docker file multiple individual stages that are building seperate container images. The end result can have a binary that has all of its dependencies statically linked, useful for example in Go containers, as it can help avoid having the Go lang toolchain in the final image. That way the build stage can be seperate from the deployable stage, and thus the deployable stage can be much smaller and we wouldn't be negatively affected as much about larger deependencies.
- Container registry - A repository or a collection of repositories used to store and access container images.

## Common CLI commands:

- docker images - Will list all images that are available locally.
- docker ps - Will list all containers that are currently running locally.
- docker ps -a - Will list all containers available locally, regardless of whether they are running or stopped.
- docker pull [image_name]:[version] - Will contact Docker Hub and will download the image named [image_name] with the version [version] locally. Docker Hub is the default location where docker pull will look for images. Without adding the [version] to the image name, docker will automatically attempt to download the latest version available, which isn't the best practice, similarly to how NPM dependencies are better managed with a defined version to avoid unexpected breaking changes after running NPM install.
- docker run [image_name]:[version/tag] - Will create and run a Docker container based off the listed image. "Tag" is an equivalent to the image's version. If we attempt to run an image that doesn't exist locally, Docker will attempt to look for the Docker image within Docker Hub, and if it finds the specified image, it'll download the image locally and then run the container based on the image downloaded. Docker run doesn't re-run previous containers even if they were created off the exact same command. Regularly, if no name is listed, Docker will generate a name randomly to the newly created container.
- docker logs [container_id/docker_name] - Will print the specified container's logs within the terminal. We can also see the container's logs through the Docker Desktop application.
- docker stop [container_id/docker_name] - Stops the specified container from running. We can stop multiple containers with a single command if desired.
- docker start [container_id/docker_name] - Runs an already existing container that was stopped previously, as opposed to docker run which creates a new container every time.
- docker build [path] - Builds a Docker image from a Dockerfile. The path is the location of the Dockerfile we want to base our Docker container off. If the Dockerfile is in the current direction, the path is simply a dot - ..
- docker attach [cotainer_id/docker_name] - Will attach the specified Docker container to the running shell/terminal.
- docker volume create [volume_name] - Creates a Docker volume with the specified name.
- docker run -v/--mount source=[volume_name],destination=[path] [image_name] - Will mount the volume into the running container and will place it in the given path within the file system.
- docker run --mount type=bind,source[volume_name],destination=[path] [image_name] - Will use bind mount to bind the data to the container instead of mounting a volume. Can be used for hot reloading on local development. For example, in docker compose: type: bind, source: [local_folder],target=[docker_folder] will bind the files in the [local_folder] to the [docker_folder], thus changes in the local folder will automatically be displayed in the docker folder. If we want certain things not to be binded, even if they exist within the [local_folder], we can specify another volume mount that only has target without a source, which would focus only on the things that we would like not to bind. By doing so, we will override the bind mount for that particular path. (Can be used to avoid binding the node modules, the build folder or other things to the Docker container).
- docker image ls | grep [image_name] - Will list all containers that use the specified [image_name] in the terminal.
- LABEL [key]=[value] - Lets add important metadata within the image's manifest. Any arbitary [key]=[value] pair can be added.
- docker network ls - Will list all different networks on our system.
- docker network create [network_name] - Will create a new network named [network_name].
- docker container inspect [container_id] | grep [network_name] - Will list the networks with [network_name] in which the container with the listed [container_id] is running.
- watch "docker ps" - Will work like regular docker ps but will refresh the data of the listed containers.
- docker volume inspect [volume_name] - To look at the details of the written [volume_name].
- docker volume ls - Will list all Docker volumes available on the machine, including annonymous volumes that are created and managed by Docker.
- docker exec -it [container_id] sh - Will let the current terminal connect into the docker container's shell command. Pressing CTRL + D is used to exit it.

## Flags:

- By adding the flag -d (detached) to docker run, we can make the Docker container run in the background without it blocking the current terminal we are using. Using it with the detached flag will print the container's container id which we can use later for other commands.
- By adding the flag -p/--publish to docker run, we can publish a container's port to the host. In order to publish the desired container, we need to check what's the current port the container is running in its isolated Docker network, which can is part of the information listed by using docker ps, and then run the following combination - -p [published_port]:[isolated_port]. So, for example, if we run docker run -d -p 9000:[isolated_port] [image_name]:[version/tag], we'll be able to access the container's running application is localhost:9000. Keep in mind, that only 1 service can run on a specific port on the host. If we attempt to run multiple containers of the same image, we'd have to associate different ports for each image. Its standard to use the same port on the host simlarly to what the container was using, when we want to use specific services that we don't want to run multiple of. (Keep in mind, this is not a requirement, just a common practice).
- By adding the flag --name to docker run, we can give Docker container a specific name. Regularly, if no name is listed, Docker will generate a name randomly to the newly created container. (Can't have two containers with the same name on a system).
- By adding the flag -t/--tag to docker build, we can give our container a specific name and a specific version/tag [container_name]:[version/tag]
- By adding -e/--env to docker run, we can add environment variables to the container we are attempting to run.
- By adding --env-file [path] - Will add environment variables to the container according to the variables listed within the env file.
- By adding -i/--interactive to docker run, we are allowed to send commands to the container through standard input. Adding -t/--tty will let us get a running shell within the container. (Can be combined to the -it flag).
- By adding --rm to docker run, we tell Docker that once the current container process is stopped, it should not store the stopped container, and instead should remove it from our system.
- By adding the --privileged flag, the container is given all capabilities and lifts all limitations enforced by the device cgroup controller.
- By adding --entrypoint [command], the entrypoint listed in the Dockerfile will be overridden by the value of the flag.
- By adding --init, Docker will run its own initialization script, and the actual process will be spawned as a subprocess. Its useful if the application is managing sub proccesses itself, thus passing the init flag can make it easier to handle forwarding along termination signals from Docker to the container.
- By adding --mount/--volume/-v - Will let us persist data outside of the container layer in a volume.
- By adding --network/-net - Will let us connect to a specific Docker network. Specifying networks like this will allow us to create isolated networks for any of our applications, rather than just letting all of the Docker containers end up on Docker's default network in such a way that they would be able to communicate with each other.
- By adding --platform - Allows us to specify which architecture we want to run our container image on. If, for example, we are using a AMD 64 system, we can run the container image with Linux ARM64 v8 option that way.
- By adding --restart [always/unless-stopped/never] - By picking one of these options, Docker will behave accordingly, unless-stopped or always will make Docker always attempt to restart a container once it closes over and over.
- By adding --cap-add or --cap-drop - Allows to specify which Linux capabilities should be accessible from the container, this is a security feature, where we can dictate what is allowable within the context of the container.
- By adding --cgroup-parent - Allows to specify which C group ID the container should be associated with in order to monitor and limit the resources the container has access to. (This is one way to specify that directly).
- By adding --cpu-shares - Another C group related option, allows to specify what percentage of the CPU cycles this container should be able to access.
- By adding --cpuset-cpus - Allows to specify which cores specifically to use if we want to fine-tune performance and want to pin a process to only execute on a specific CPU core.
- By adding --device-cgroup-rule/--device-read-bps/--device-read-iops/--device-write-bps/--device-write-iops - Related to the different devices that the container should have access to and what throughput and bandwidth.
- By adding --gpus - Allows to access GPUs with the container. (NVIDIA only).
- By adding --health-cmd/--health-interval/--health-retries/--health-start-period/--health-timeout - We can use these health related options to specify a health check that Docker will use to periodically ping the container and make sure its healthy.
- By adding --memory/-m - Like the CPU options, we can use this option to specify how much memory the container process should have access to. If it goes over the specified limit, Docker will kill it.
- By adding --pid/--pid-limit - To specifiy how many sub processes the container should be allowed to manage.
- By adding --priviledged - Overrides all of the other security options and effictively gives the container access to all of the priviledges that it could want.
- By adding --read-only - Specifies that even the container layer of the file system should be read only, so if write access isn't needed, this can be another security precaution.
- By adding --security-opt - Allows to specifiy either app armor set comp profiles that are desired to be used for the container when its run.
- By adding dockerd --userns-remap - Is not something that is executed at container runtime, but is something that you specify you're running dockerd, for example if installing Docker engine, and that is to enable namespace remapping for the user's namespace, which allows to map from a non-root user on the host system to a root user inside of the containers. That's one more layer of defense against a potential attacker.
- By adding -f [path] - Docker is told the required Dockerfile is specified in [path].
- By adding to any command --help, the CLI will list all possible options with short explanations for each addition to the current command.
- By adding prune - All unused images/containers/volumes/network will be removed (depending on the specification prior to prune within the command, as in docker image / docker container / docker volume / docker network).

## Common Dockerfile commands

- FROM [image_name]:[version/tag] - Bases the current Docker image we are building off a base image mentioned next to a RUN command. Adding "as [label]" lets us label a stage for multi stage builds. That way, we can use COPY --from=[label] to use a pervious stage in the current one.
- RUN [command] - Will execute any command in a shell inside the container environment. Most Docker images are Linux based, so it is possible to run Linux commands inside the container aswell, alongside other base commands such as npm install, assuming the Docker environment has node and npm installed within it.
- COPY [files] [destination] - Copies the picked files from the current folder/designated folder [src_folder] to the filesystem of at container as the path [destination]. We can also copy entire directories of files to the container. If we add [destination]/, the Docker knows to create the folder if it doesn't exist in the container yet. /[destination] tells Docker we want to the destination to exist on the root of the Linux file system. If we want to copy everything, the files can be described as a dot - ., and if we want the container to have the exact same file system, we can also describe the destination as a dot - ..
- WORKDIR [directory] - Sets the working directory for all following commands. (Similarly to changing directory with cd [directory]).
- CMD [command, parameter] - The instruction that is to be executed when a Docker container starts. That is the last command of the Dockerfile and should be the one that starts the application. There can only be one "CMD" instruction in a Dockerfile.
- RUN apt update - Updates metadata of the package manager in order to find the correct package without listing specific versions. This is mainly used to install the appropriate packages for the environment, such as nodejs. (It would be better to use official package environment images and install specific versions instead).
- USER [name] - Uses the user [name] to execute the commands from the specified command. The nodejs image has the user node.
- COPY --chown=[user]:[user] [files] [destination] - Will copy the listed files owned by [user] to the Docker image. These files will be owned by the [user] and accessible by that [user].
- ENV [variable] - Sets environment variables. For example, nodejs applications look for NODE_ENV, so ENV NDOE_ENV production will set NODE_ENV to be production, that way we can control how the application behaves in environment compared to development mode.
- EXPOSE [port_number] - This command doesn't actually change how the image behaves. Its just a documentation for users of the image, that the image is expected to run on [port_number].
- RUN --mount=type=cache.target=[workdir_path]/.npm \
   npm set cache [work_dir]/.npm && \
   npm ci --only=production - Will mount cache location to store information from the step specifically. This is a buildkit feature. Buildkit is required to be enabled and DOCKER_BUILDKIT needs to be set to 1 if we want to use this command.
- RUN --mount=type=secret,[key]=[value],dst=[destination] - Will mount secret to the file system of the container. Will only exist in build time, will not be included in the final artifact. Important technique incase credentials are needed within the build context.
- ENTRYPOINT [command, paramter] - If we are building our own application with the same set of arguments, CMD is the way to go. However, if we want to create a utility where we want to pass additional arguments to the container at run time, we want to place the executable at entry point, and allow people to append arguments via the command at run time.
- ADD [files] [destination] - Performs similarly to COPY, Docker prefers the COPY command in general. There are just specific changes in certain edgecases. Its better to use COPY in all scenarios.
- buildx - Built into the Docker CLI. Always to build and run multi-architecture images. If one person is on a x86 base system and want to deploy to an ARM base system, buildx can be used to build versions of an image that are compatible with either of those.
- ARG [key]=[value] - Setting an environment variable that is available at build time but not at runtime. They still exist in the image metadata

## Dockerfile suggestions

- Copying package.json and running npm install before copying anything else helps Docker cache the dependencies. That way, the package.json layer will only be invalidated if the dependencies are changed.
- Its a best practice to specify the working directory within the image with WORKDIR.
- One of the techniques to improve security through the Dockerfile is to execute the commands as a non-root user.
- For production images, set the appropriate ENV production variables, also, for node based images, npm ci --only=production will be more ideal, as it won't install dev dependencies, and would install the specific versions listed in the package-lock file.
- Its possible to add different development environment and production environment commands under the same file. When specifiying for which environment, we add to the FROM command "as [environment]", and then when building the container image, we need to specify which stages of the multi stage build we would like to target.
- Add FROM [previous_stage] if, during a new build stage, we want to continue from one of the previous ones.

## Docker compose

- Docker compose is another way to run containers as opposed to docker run. Behind the scenes they do exactly the same thing, but docker compose allows to specificy all of the application's configurations within a yaml file and it makes dealing with a container's lifecycle much easier.
- composerize is a GitHub repository which we can paste in a docker run command and it will generate the equivalent docker compose command format. (Didn't test it out or investigate how safe it is, wouldn't necessarily recommend it without checking).
- To run docker compose, we use the docker compose command, and then we can attach flags to it similarly to docker run. For example, docker compose up -d will run the docker compose's commands in a detached terminal, similarly to docker run -d.
- If the docker compose file isn't called docker-compose.yml, or it isn't in the same folder, we'd have to specifiy the name and path similarly to how we use docker run.
- Its possible to take multiple docker compose files and pass them to the docker compose up command. It will take all files and overlay tham. The only thing that has to be done is to define the fields that change between them - such as having the same base configuration and add in the differences between them.

## Docker security

- Just because an application is containered, doesn't mean it is secured.
- If we specify USER=root, the --priviledged option and connect to the host network through --net=host, almost all of the security boundaries the container would provide will be removed.
- There are two primary components for container security. The first one is the security of the image itself - while building the image, what features are being used and how they are configured will dictate how secure the image is. A big part of that are the software dependencies that are installed within the image, and what attack surface area exists that a potential hacker could exploit. The other one is at runtime. If someone were to compromise the container because of some vulnerability that was built in, it will affect what they would actually be able to do. Would they be able to escape the container and move laterally to compromise the host or other containers? Or will they mostly be stuck and confined within the container because the runtime is configured apropriately?
- Some of the main things that should be considered around image security are how to keep the attack surface area as small as possible. One way to do that, is to use minimal base images, so the smaller the image, the fewer things installed in it, the less likelyhood there is a potential bug that can be compromised. (Chainguard is a company that revolves around securing base images, which maintains a set of base images with security focus - wasn't checked personally). As writing the Dockerfile, don't include things in the final image that aren't needed. If its not needed in production time, don't install it, or, use a multi stage build if you need it at build time but not at production time. Also, after building the image, there are a number of tools available that can be used to scan the image for potential vulnerabilities. One built in to Docker called Sneak (Wasn't personally checked), another from Aqua Security called Trivi (Wasn't personally checked) - we can take advantage of these tools to scan images and see if there are potential vulnerabilities and what are the levels of said vulnerabilities. That gives visibility and allows to choose if patches are needed or not. Ideally, we don't want to use a root user inside a container but a Linux user with a minimal set of permissions to accomplish the desired tasks. We should avoid building credentials into the image itself. Its best practice to treat the images as if they are public images in order to think about what information shouldn't be available at runtime. Its also possible to cryptographically sign the images to prove who was the person who built them, which can be useful to verify that a specific image is exactly what is expected. When choosing which base images to use, make sure to pin atleast to the minor version number. If pinning to the major.minor but not to the patch version it would allow updates to the patch version (like for bug fixes) to automatically roll in but hopefully not incorporate breaking changes into the system. Alternatively, if we want to lock down the base image, we can use the image's hash which will always refer to the same image.
- On the runtime side, we can think about the container runtime itself and the configuration options used for each container. If we are setting up the Docker Daemon (Dockerd), within Docker engine, one good tip is to use the user namespace remap option (Documented in the Docker docs) to ensure the containers being run at a seperate user namespace than the host system, and for individual containers we can set the file system as read-only if the application doesn't require write access. We can use the cap-drop all option to remove all capabilities and add back anything that we might need after that. We can limit the CPU and memory to prevent a denial of service situation where one process is using too much of the resouces, and we can use security options to set either set comp profiles or app armor profiles. (Both are documented in the Docker docs).

## Linux operating system commands

- ls - Will print the current folder's files' details.
- cat [file_path/file_name] - Will print the file's text inside the terminal, assuming the file was found in the specified path.
- rm [file_path/file_name] - Will remove the file from the file system if the file was found in the specified path.
- echo [text] > [file_name] - Will place the text in file_name and will create the file if it doesn't exist.
- vi [file_name] - Opens or edits the file if found.
